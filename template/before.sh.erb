# Find available port to run server on
port=$(find_port ${host})

export GRADIO_SERVER_PORT=${port}
export GRADIO_SERVER_NAME="0.0.0.0"
export GRADIO_DEBUG=1
export GRADIO_ROOT_PATH="/node/${host}/${port}"
export FORWARDED_ALLOW_IPS="127.0.0.1"

################################################################################

export APP="${PWD}/app.py"
export HF_HOME="/scratch/${SLURM_JOB_ID}"
export MODEL="${HOME}/llm/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

################################################################################

(
umask 077
cat > "$APP" << EOL

print("Starting Python script...")

print("Importing gradio...")
import gradio as gr

print("Importing os...")
import os

print("Importing llama_cpp...")
import llama_cpp
print("llama_cpp imported successfully!")

MODEL = os.environ.get("MODEL", os.path.expanduser("~/llm/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"))
print(f"Model path: {MODEL}")

# Check for GPU availability and configure accordingly
try:
    print("Attempting to load model with GPU...")
    # Try GPU-accelerated loading
    llama = llama_cpp.Llama(
        model_path=MODEL, 
        n_gpu_layers=-1,  # Use all GPU layers
        n_ctx=4096, 
        verbose=False,
        n_threads=16,
        n_batch=768,
        use_mlock=True,
        low_vram=False
    )
    print("GPU acceleration enabled")
except Exception as e:
    print(f"GPU failed, falling back to CPU: {e}")
    # Fallback to CPU if GPU fails
    llama = llama_cpp.Llama(
        model_path=MODEL, 
        n_gpu_layers=0, 
        n_ctx=2048, 
        verbose=True,
        use_mlock=True
    )

model = "Meta-Llama-3.1-8B-Instruct-Q4_K_M"

def predict(message, history):
    messages = []
    for user_message, assistant_message in history:
        messages.append({"role": "user", "content": user_message})
        messages.append({"role": "assistant", "content": assistant_message})
    messages.append({"role": "user", "content": message})

    response = llama.create_chat_completion_openai_v1(model=model, messages=messages, stream=True)

    text = ""
    for chunk in response:
        content = chunk.choices[0].delta.content
        if content:
            text += content
            yield text

with gr.Blocks(theme=gr.themes.Soft(), fill_height=True, head='<link rel="icon" href="/favicon.ico">') as demo:
    gr.ChatInterface(
        predict,
        fill_height=True,
        examples=[
            "What year is it?",
            "Where is UAB?",
            "What is Open OnDemand?",
            "How many r's are in the word strawberry?",
        ],
    )

if __name__ == "__main__":
    print("Launching Gradio app...")
    demo.launch()
EOL
)
